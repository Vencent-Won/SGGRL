

import os
import csv
import math
import torch
import pickle
import random

import numpy as np
import torch.nn as nn
import torch.nn.functional as F

from tqdm import tqdm
from rdkit import Chem, rdBase
from typing import Callable, List, Union, Tuple
from os.path import join, exists
from argparse import Namespace
from utils.enumerator import SmilesEnumerator
from chemprop.features import get_features_generator
from chemprop.data.scaler import StandardScaler
from pgl.utils.data import Dataloader
from torch_geometric.data import Data, DataLoader, Batch
from chemprop.data.scaffold import log_scaffold_stats, scaffold_split
from torch.utils.data.dataset import Dataset

rdBase.DisableLog('rdApp.*')

PAD = 0
MAX_LEN = 220


__all__ = ['InMemoryDataset']


class MoleculeDatapoint:
    """A MoleculeDatapoint contains a single molecule and its associated features and targets."""

    def __init__(self,
                 line: List[str],
                 args: Namespace = None,
                 idx: int = None,
                 features: np.ndarray = None,
                 use_compound_names: bool = False):
        """
        Initializes a MoleculeDatapoint, which contains a single molecule.

        :param line: A list of strings generated by separating a line in a data CSV file by comma.
        :param args: Arguments.
        :param features: A numpy array containing additional features (ex. Morgan fingerprint).
        :param use_compound_names: Whether the data CSV includes the compound name on each line.
        """
        if args is not None:
            self.features_generator = args.features_generator
            self.args = args
        else:
            self.features_generator = self.args = None

        if features is not None and self.features_generator is not None:
            raise ValueError('Currently cannot provide both loaded features and a features generator.')
        self.idx = idx
        self.features = features

        if use_compound_names:
            self.compound_name = line[0]  # str
            line = line[1:]
        else:
            self.compound_name = None

        self.smiles = line[0]  # str
        self.mol = Chem.MolFromSmiles(self.smiles)

        # Generate additional features if given a generator
        if self.features_generator is not None:
            self.features = []

            for fg in self.features_generator:
                features_generator = get_features_generator(fg)
                if self.mol is not None and self.mol.GetNumHeavyAtoms() > 0:
                    self.features.extend(features_generator(self.mol))

            self.features = np.array(self.features)

        # Fix nans in features
        if self.features is not None:
            replace_token = 0
            self.features = np.where(np.isnan(self.features), replace_token, self.features)

        # Create targets
        self.targets = [float(x) if x != '' else None for x in line[1:]]

    def set_features(self, features: np.ndarray):
        """
        Sets the features of the molecule.

        :param features: A 1-D numpy array of features for the molecule.
        """
        self.features = features

    def num_tasks(self) -> int:
        """
        Returns the number of prediction tasks.

        :return: The number of tasks.
        """
        return len(self.targets)

    def set_targets(self, targets: List[float]):
        """
        Sets the targets of a molecule.

        :param targets: A list of floats containing the targets.
        """
        self.targets = targets

    def idx(self) -> int:

        return self.idx


class MoleculeDataset(Dataset):
    """A MoleculeDataset contains a list of molecules and their associated features and targets."""

    def __init__(self, data: List[MoleculeDatapoint]):
        """
        Initializes a MoleculeDataset, which contains a list of MoleculeDatapoints (i.e. a list of molecules).

        :param data: A list of MoleculeDatapoints.
        """
        self.data = data
        self.args = self.data[0].args if len(self.data) > 0 else None
        self.scaler = None

    def compound_names(self) -> List[str]:
        """
        Returns the compound names associated with the molecule (if they exist).

        :return: A list of compound names or None if the dataset does not contain compound names.
        """
        if len(self.data) == 0 or self.data[0].compound_name is None:
            return None

        return [d.compound_name for d in self.data]

    def smiles(self) -> List[str]:
        """
        Returns the smiles strings associated with the molecules.

        :return: A list of smiles strings.
        """
        return [d.smiles for d in self.data]

    def mols(self) -> List[Chem.Mol]:
        """
        Returns the RDKit molecules associated with the molecules.

        :return: A list of RDKit Mols.
        """
        return [d.mol for d in self.data]

    def features(self) -> List[np.ndarray]:
        """
        Returns the features associated with each molecule (if they exist).

        :return: A list of 1D numpy arrays containing the features for each molecule or None if there are no features.
        """
        if len(self.data) == 0 or self.data[0].features is None:
            return None

        return [d.features for d in self.data]

    def targets(self) -> List[List[float]]:
        """
        Returns the targets associated with each molecule.

        :return: A list of lists of floats containing the targets.
        """
        return [d.targets for d in self.data]

    def num_tasks(self) -> int:
        """
        Returns the number of prediction tasks.

        :return: The number of tasks.
        """
        return self.data[0].num_tasks() if len(self.data) > 0 else None

    def features_size(self) -> int:
        """
        Returns the size of the features array associated with each molecule.

        :return: The size of the features.
        """
        return len(self.data[0].features) if len(self.data) > 0 and self.data[0].features is not None else None

    def shuffle(self, seed: int = None):
        """
        Shuffles the dataset.

        :param seed: Optional random seed.
        """
        if seed is not None:
            random.seed(seed)
        random.shuffle(self.data)

    def normalize_features(self, scaler: StandardScaler = None, replace_nan_token: int = 0) -> StandardScaler:
        """
        Normalizes the features of the dataset using a StandardScaler (subtract mean, divide by standard deviation).

        If a scaler is provided, uses that scaler to perform the normalization. Otherwise fits a scaler to the
        features in the dataset and then performs the normalization.

        :param scaler: A fitted StandardScaler. Used if provided. Otherwise a StandardScaler is fit on
        this dataset and is then used.
        :param replace_nan_token: What to replace nans with.
        :return: A fitted StandardScaler. If a scaler is provided, this is the same scaler. Otherwise, this is
        a scaler fit on this dataset.
        """
        if len(self.data) == 0 or self.data[0].features is None:
            return None

        if scaler is not None:
            self.scaler = scaler

        elif self.scaler is None:
            features = np.vstack([d.features for d in self.data])
            self.scaler = StandardScaler(replace_nan_token=replace_nan_token)
            self.scaler.fit(features)

        for d in self.data:
            d.set_features(self.scaler.transform(d.features.reshape(1, -1))[0])

        return self.scaler

    def set_targets(self, targets: List[List[float]]):
        """
        Sets the targets for each molecule in the dataset. Assumes the targets are aligned with the datapoints.

        :param targets: A list of lists of floats containing targets for each molecule. This must be the
        same length as the underlying dataset.
        """
        assert len(self.data) == len(targets)
        for i in range(len(self.data)):
            self.data[i].set_targets(targets[i])

    def sort(self, key: Callable):
        """
        Sorts the dataset using the provided key.

        :param key: A function on a MoleculeDatapoint to determine the sorting order.
        """
        self.data.sort(key=key)

    def idx(self):
        return [temp.idx for temp in self.data]

    def __len__(self) -> int:
        """
        Returns the length of the dataset (i.e. the number of molecules).

        :return: The length of the dataset.
        """
        return len(self.data)

    def __getitem__(self, item) -> Union[MoleculeDatapoint, List[MoleculeDatapoint]]:
        """
        Gets one or more MoleculeDatapoints via an index or slice.

        :param item: An index (int) or a slice object.
        :return: A MoleculeDatapoint if an int is provided or a list of MoleculeDatapoints if a slice is provided.
        """
        return self.data[item]


def filter_invalid_smiles(data: MoleculeDataset) -> MoleculeDataset:
    """
    Filters out invalid SMILES.

    :param data: A MoleculeDataset.
    :return: A MoleculeDataset with only valid molecules.
    """
    return MoleculeDataset([datapoint for datapoint in data
                            if datapoint.smiles != '' and datapoint.mol is not None
                            and datapoint.mol.GetNumHeavyAtoms() > 0])


def get_data(args, path: str, skip_invalid_smiles: bool = True) -> MoleculeDataset:
    """
    Converts SMILES to a MoleculeDataset.

    :param smiles: A list of SMILES strings.
    :param skip_invalid_smiles: Whether to skip and filter out invalid smiles.
    :param logger: Logger.
    :return: A MoleculeDataset with all of the provided SMILES.
    """
    use_compound_names = False

    max_data_size = float('inf')

    # Load features
    features_data = None

    skip_smiles = set()
    max_len = 10
    # Load data
    with open(path) as f:
        reader = csv.reader(f)
        next(reader)  # skip header
        lines = []
        for line in reader:
            smiles = line[0]
            if smiles in skip_smiles:
                continue
            if len(smiles) > max_len:
                max_len = len(smiles)
            lines.append(line)
            if len(lines) >= max_data_size:
                break

        data = MoleculeDataset([
            MoleculeDatapoint(line=line,
                              args=args,
                              idx=i,
                              features=features_data[i] if features_data is not None else None,
                              use_compound_names=use_compound_names
                              ) for i, line in tqdm(enumerate(lines), total=len(lines))
        ])

    # Filter out invalid SMILES
    if skip_invalid_smiles:
        original_data_len = len(data)
        data = filter_invalid_smiles(data)

        if len(data) < original_data_len:
            print(f'Warning: {original_data_len - len(data)} SMILES are invalid.')

    return data, max_len + 10


def split_data(data: MoleculeDataset,
               split_type: str = 'random',
               sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1),
               seed: int = 0,
               args: Namespace = None) -> Tuple[MoleculeDataset,
                                                MoleculeDataset,
                                                MoleculeDataset]:
    """
    Splits data into training, validation, and test splits.

    :param data: A MoleculeDataset.
    :param split_type: Split type.
    :param sizes: A length-3 tuple with the proportions of data in the
    train, validation, and test sets.
    :param seed: The random seed to use before shuffling data.
    :param args: Namespace of arguments.
    :param logger: A logger.
    :return: A tuple containing the train, validation, and test splits of the data.
    """
    assert len(sizes) == 3 and sum(sizes) == 1

    if args.folds_file is not None:
        folds_file, val_fold_index, test_fold_index = \
            args.folds_file, args.val_fold_index, args.test_fold_index
    else:
        folds_file = val_fold_index = test_fold_index = None

    if split_type == 'crossval':
        print('=' * 45, 'crossval', '=' * 45)
        index_set = args.crossval_index_sets[args.seed]
        data_split = []
        for split in range(3):
            split_indices = index_set[split]
            # =============================================================================
            #             split_indices = []
            #             for index in index_set[split]:
            #                 with open(os.path.join(args.crossval_index_dir, f'{index}.pkl'), 'rb') as rf:
            #                     split_indices.extend(pickle.load(rf))
            # =============================================================================
            data_split.append([data[i] for i in split_indices])
        train, val, test = tuple(data_split)
        print(f'train size: {len(train)}, val size: {len(val)}, test size: {len(test)}')
        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)

    elif split_type == 'index_predetermined':
        split_indices = args.crossval_index_sets[args.seed]
        assert len(split_indices) == 3
        data_split = []
        for split in range(3):
            data_split.append([data[i] for i in split_indices[split]])
        train, val, test = tuple(data_split)
        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)

    elif split_type == 'predetermined':
        if not val_fold_index:
            assert sizes[2] == 0  # test set is created separately so use all of the other data for train and val
        assert folds_file is not None
        assert test_fold_index is not None

        try:
            with open(folds_file, 'rb') as f:
                all_fold_indices = pickle.load(f)
        except UnicodeDecodeError:
            with open(folds_file, 'rb') as f:
                all_fold_indices = pickle.load(f, encoding='latin1')  # in case we're loading indices from python2
        # assert len(data) == sum([len(fold_indices) for fold_indices in all_fold_indices])

        log_scaffold_stats(data, all_fold_indices)

        folds = [[data[i] for i in fold_indices] for fold_indices in all_fold_indices]

        test = folds[test_fold_index]
        if val_fold_index is not None:
            val = folds[val_fold_index]

        train_val = []
        for i in range(len(folds)):
            if i != test_fold_index and (val_fold_index is None or i != val_fold_index):
                train_val.extend(folds[i])

        if val_fold_index is not None:
            train = train_val
        else:
            random.seed(seed)
            random.shuffle(train_val)
            train_size = int(sizes[0] * len(train_val))
            train = train_val[:train_size]
            val = train_val[train_size:]

        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)

    elif split_type == 'scaffold_balanced':
        return scaffold_split(data, sizes=sizes, balanced=True, seed=seed)
    elif split_type == 'new_scaffold_balanced':
        return scaffold_similarity_split(data, sizes=sizes, balanced=True, seed=seed)

    elif split_type == 'random':
        idx = list(range(len(data)))
        np.random.seed(seed)
        np.random.shuffle(idx)

        train_size = int(sizes[0] * len(data))
        train_val_size = int((sizes[0] + sizes[1]) * len(data))

        train = MoleculeDataset([data[i] for i in idx[:train_size]])
        val = MoleculeDataset([data[i] for i in idx[train_size:train_val_size]])
        test = MoleculeDataset([data[i] for i in idx[train_val_size:]])

        return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)

    else:
        raise ValueError(f'split_type "{split_type}" not supported.')


# Split SMILES into words
def split(sm):
    '''
    function: Split SMILES into words. Care for Cl, Br, Si, Se, Na etc.
    input: A SMILES
    output: A string with space between words
    '''
    arr = []
    i = 0
    while i < len(sm) - 1:
        if not sm[i] in ['%', 'C', 'B', 'S', 'N', 'R', 'X', 'L', 'A', 'M', \
                         'T', 'Z', 's', 't', 'H', '+', '-', 'K', 'F']:
            arr.append(sm[i])
            i += 1
        elif sm[i] == '%':
            arr.append(sm[i:i + 3])
            i += 3
        elif sm[i] == 'C' and sm[i + 1] == 'l':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'C' and sm[i + 1] == 'a':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'C' and sm[i + 1] == 'u':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'B' and sm[i + 1] == 'r':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'B' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'B' and sm[i + 1] == 'a':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'B' and sm[i + 1] == 'i':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'S' and sm[i + 1] == 'i':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'S' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'S' and sm[i + 1] == 'r':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'N' and sm[i + 1] == 'a':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'N' and sm[i + 1] == 'i':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'R' and sm[i + 1] == 'b':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'R' and sm[i + 1] == 'a':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'X' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'L' and sm[i + 1] == 'i':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'A' and sm[i + 1] == 'l':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'A' and sm[i + 1] == 's':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'A' and sm[i + 1] == 'g':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'A' and sm[i + 1] == 'u':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'M' and sm[i + 1] == 'g':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'M' and sm[i + 1] == 'n':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'T' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'Z' and sm[i + 1] == 'n':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 's' and sm[i + 1] == 'i':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 's' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 't' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'H' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '+' and sm[i + 1] == '2':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '+' and sm[i + 1] == '3':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '+' and sm[i + 1] == '4':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '-' and sm[i + 1] == '2':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '-' and sm[i + 1] == '3':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == '-' and sm[i + 1] == '4':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'K' and sm[i + 1] == 'r':
            arr.append(sm[i:i + 2])
            i += 2
        elif sm[i] == 'F' and sm[i + 1] == 'e':
            arr.append(sm[i:i + 2])
            i += 2
        else:
            arr.append(sm[i])
            i += 1
    if i == len(sm) - 1:
        arr.append(sm[i])
    return ' '.join(arr)


# 活性化関数
class GELU(nn.Module):
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))


# 位置情報を考慮したFFN
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = GELU()

    def forward(self, x):
        return self.w_2(self.dropout(self.activation(self.w_1(x))))


# 正規化層
class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2


class SublayerConnection(nn.Module):
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))


# Sample SMILES from probablistic distribution
def sample(msms):
    ret = []
    for msm in msms:
        ret.append(torch.multinomial(msm.exp(), 1).squeeze())
    return torch.stack(ret)


def validity(smiles):
    loss = 0
    for sm in smiles:
        mol = Chem.MolFromSmiles(sm)
        if mol is None:
            loss += 1
    return 1 - loss / len(smiles)


class Randomizer(object):

    def __init__(self):
        self.sme = SmilesEnumerator()

    def __call__(self, sm):
        sm_r = self.sme.randomize_smiles(sm)  # Random transoform
        if sm_r is None:
            sm_spaced = split(sm)  # Spacing
        else:
            sm_spaced = split(sm_r)  # Spacing
        sm_split = sm_spaced.split()
        if len(sm_split) <= MAX_LEN - 2:
            return sm_split  # List
        else:
            return split(sm).split()

    def random_transform(self, sm):
        '''
        function: Random transformation for SMILES. It may take some time.
        input: A SMILES
        output: A randomized SMILES
        '''
        return self.sme.randomize_smiles(sm)



class Seq2seqDataset(Dataset):

    def __init__(self, smiles, vocab, seq_len=220, device=None, transform=Randomizer()):
        self.smiles = smiles
        self.vocab = vocab
        self.seq_len = seq_len
        self.transform = transform
        self.device = device

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, item):
        sm = self.smiles[item]
        # sm = self.transform(sm) # List
        content = [self.vocab.stoi.get(token, self.vocab.unk_index) for token in sm]
        X = [self.vocab.sos_index] + content + [self.vocab.eos_index]
        padding = [self.vocab.pad_index] * (self.seq_len - len(X))
        X.extend(padding)
        X = torch.tensor(X).to(self.device)
        X_one_hot = F.one_hot(X, len(self.vocab)).float()
        return X, X_one_hot



def mp_pool_map(list_input, func, num_workers):
    """list_output = [func(input) for input in list_input]"""
    class _CollateFn(object):
        def __init__(self, func):
            self.func = func
        def __call__(self, data_list):
            new_data_list = []
            for data in data_list:
                index, input = data
                new_data_list.append((index, self.func(input)))
            return new_data_list

    # add index
    list_new_input = [(index, x) for index, x in enumerate(list_input)]
    data_gen = Dataloader(list_new_input,
                          batch_size=8,
                          num_workers=num_workers,
                          shuffle=False,
                          collate_fn=_CollateFn(func))

    list_output = []
    for sub_outputs in data_gen:
        list_output += sub_outputs
    list_output = sorted(list_output, key=lambda x: x[0])
    # remove index
    list_output = [x[1] for x in list_output]
    return list_output


def save_data_list_to_npz(data_list, npz_file):
    """
    Save a list of data to the npz file. Each data is a dict
    of numpy ndarray.

    Args:
        data_list(list): a list of data.
        npz_file(str): the npz file location.
    """
    keys = data_list[0].keys()
    merged_data = {}
    for key in keys:
        if len(np.array(data_list[0][key]).shape) == 0:
            lens = np.ones(len(data_list)).astype('int')
            values = np.array([data[key] for data in data_list])
            singular = 1
        else:
            lens = np.array([len(data[key]) for data in data_list])
            values = np.concatenate([data[key] for data in data_list], 0)
            singular = 0
        merged_data[key] = values
        merged_data[key + '.seq_len'] = lens
        merged_data[key + '.singular'] = singular
    np.savez_compressed(npz_file, **merged_data)


def load_npz_to_data_list(npz_file):
    """
    Reload the data list save by ``save_data_list_to_npz``.

    Args:
        npz_file(str): the npz file location.

    Returns:
        a list of data where each data is a dict of numpy ndarray.
    """
    def _split_data(values, seq_lens, singular):
        res = []
        s = 0
        for l in seq_lens:
            if singular == 0:
                res.append(values[s: s + l])
            else:
                res.append(values[s])
            s += l
        return res

    merged_data = np.load(npz_file, allow_pickle=True)
    names = [name for name in merged_data.keys()
            if not name.endswith('.seq_len') and not name.endswith('.singular')]
    data_dict = {}
    for name in names:
        data_dict[name] = _split_data(
                merged_data[name],
                merged_data[name + '.seq_len'],
                merged_data[name + '.singular'])

    data_list = []
    n = len(data_dict[names[0]])
    for i in range(n):
        data = {name: data_dict[name][i] for name in names}
        data_list.append(data)
    return data_list

class InMemoryDataset(object):
    """
    Description:
        The InMemoryDataset manages ``data_list`` which is a list of `data` and
        the `data` is a dict of numpy ndarray. And each dict has the same keys.

        It works like a list: you can call `dataset[i] to get the i-th element of
        the ``data_list`` and call `len(dataset)` to get the length of ``data_list``.

        The ``data_list`` can be cached in npz files by calling `dataset.save_data(data_path)`
        and after that, call `InMemoryDataset(data_path)` to reload.

    Attributes:
        data_list(list): a list of dict of numpy ndarray.

    Example:
        .. code-block:: python

            data_list = [{'a': np.zeros([4, 5])}, {'a': np.zeros([7, 5])}]
            dataset = InMemoryDataset(data_list=data_list)
            print(len(dataset))
            dataset.save_data('./cached_npz')   # save data_list to ./cached_npz

            dataset2 = InMemoryDataset(npz_data_path='./cached_npz')    # will load the saved `data_list`
            print(len(dataset))
    """

    def __init__(self,
                 data_list=None,
                 npz_data_path=None,
                 npz_data_files=None):
        """
        Users can either directly pass the ``data_list`` or pass the `data_path` from
        which the cached ``data_list`` will be loaded.

        Args:
            data_list(list): a list of dict of numpy ndarray.
            data_path(str): the path to the cached npz path.
        """
        super(InMemoryDataset, self).__init__()
        self.data_list = data_list
        self.npz_data_path = npz_data_path
        self.npz_data_files = npz_data_files

        if not npz_data_path is None:
            self.data_list = self._load_npz_data_path(npz_data_path)

        if not npz_data_files is None:
            self.data_list = self._load_npz_data_files(npz_data_files)

    def _load_npz_data_path(self, data_path):
        data_list = []
        files = [f for f in os.listdir(data_path) if f.endswith('.npz')]
        files = sorted(files)
        for f in files:
            data_list += load_npz_to_data_list(join(data_path, f))
        return data_list

    def get_data(self, device):
        atom_bond_graph_list = []
        bond_angle_graph_list = []
        node_count = 0
        atom_names = ["atomic_num", "formal_charge", "degree",
                      "chiral_tag", "total_numHs", "is_aromatic",
                      "hybridization"]
        bond_names = ["bond_dir", "bond_type", "is_in_ring"]
        bond_float_names = ["bond_length"]
        bond_angle_float_names = ['bond_angle']
        for data in self.data_list:
            ab_g = Data(edge_index=torch.LongTensor(data['edges']).T.to(device),
                        x=torch.LongTensor(np.stack([data[name] for name in atom_names])).T.to(device),
                        edge_attr=torch.FloatTensor(np.stack([data[name] for name in bond_names + bond_float_names])).T.to(device))
            ba_g = Data(edge_index=torch.LongTensor(data['BondAngleGraph_edges'].T).to(device),
                        x=torch.FloatTensor(np.stack([data[name] for name in bond_names + bond_float_names])).T.to(device),
                        edge_attr=torch.FloatTensor(np.stack([data[name] for name in bond_angle_float_names])).T.to(device))
            atom_bond_graph_list.append(ab_g)
            bond_angle_graph_list.append(ba_g)
        self.atom_bond_graph_list = atom_bond_graph_list
        self.bond_angle_graph_list = bond_angle_graph_list

    def get_batch(self, index):
        return Batch.from_data_list([self.atom_bond_graph_list[id] for id in index]), \
               Batch.from_data_list([self.bond_angle_graph_list[id] for id in index])


    def _load_npz_data_files(self, data_files):
        data_list = []
        for f in data_files:
            data_list += load_npz_to_data_list(f)
        return data_list

    def _save_npz_data(self, data_list, data_path, max_num_per_file=10000):
        if not exists(data_path):
            os.makedirs(data_path)
        n = len(data_list)
        for i in range(int((n - 1) / max_num_per_file) + 1):
            filename = 'part-%06d.npz' % i
            sub_data_list = self.data_list[i * max_num_per_file: (i + 1) * max_num_per_file]
            save_data_list_to_npz(sub_data_list, join(data_path, filename))

    def save_data(self, data_path):
        """
        Save the ``data_list`` to the disk specified by ``data_path`` with npz format.
        After that, call `InMemoryDataset(data_path)` to reload the ``data_list``.

        Args:
            data_path(str): the path to the cached npz path.
        """
        self._save_npz_data(self.data_list, data_path)

    def __getitem__(self, key):
        if isinstance(key, slice):
            start, stop, step = key.indices(len(self))
            dataset = InMemoryDataset(
                data_list=[self[i] for i in range(start, stop, step)])
            return dataset
        elif isinstance(key, int) or \
                isinstance(key, np.int64) or \
                isinstance(key, np.int32):
            return self.data_list[key]
        elif isinstance(key, list):
            dataset = InMemoryDataset(
                data_list=[self[i] for i in key])
            return dataset
        else:
            raise TypeError('Invalid argument type: %s of %s' % (type(key), key))

    def __len__(self):
        return len(self.data_list)

    def transform(self, transform_fn, num_workers=4, drop_none=False):
        """
        Inplace apply `transform_fn` on the `data_list` with multiprocess.
        """
        data_list = mp_pool_map(self.data_list, transform_fn, num_workers)
        if drop_none:
            self.data_list = [data for data in data_list if not data is None]
        else:
            self.data_list = data_list

